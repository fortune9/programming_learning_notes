Example code for using dbplyr arrow package to handle big dataset
================
Zhenguo Zhang
2026-01-11

- [Play the dbduck database](#play-the-dbduck-database)
  - [Create a database](#create-a-database)
  - [Load some data](#load-some-data)
  - [List tables in the database](#list-tables-in-the-database)
  - [Use dbplyr to interact with the
    database](#use-dbplyr-to-interact-with-the-database)
  - [Exectute sql queries](#exectute-sql-queries)
- [Access Arrow object](#access-arrow-object)
  - [Indexes or Keys](#indexes-or-keys)
- [List of useful functions from
  dbplyr](#list-of-useful-functions-from-dbplyr)
- [List of useful functions from
  DBI](#list-of-useful-functions-from-dbi)
- [List of useful functions from
  duckdb](#list-of-useful-functions-from-duckdb)
- [References](#references)

``` r
library(DBI)
library(dbplyr)
library(tidyverse)
```

This document is based on the page
<https://r4ds.hadley.nz/databases.html>

In R, there are two important packages to handle databases:

- DBI: a common low-level interface between R and database management
  systems
- dbplyr: a dplyr backend that translates your dplyr code to SQL and
  then executes them with DBI.

Databases are run by database management systems (DBMS’s for short),
which come in three basic forms:

- Client-server DBMS’s run on a powerful central server, which you
  connect to from your computer (the client). They are great for sharing
  data with multiple people in an organization. Popular client-server
  DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.
- Cloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s
  BigQuery, are similar to client server DBMS’s, but they run in the
  cloud. This means that they can easily handle extremely large datasets
  and can automatically provide more compute resources as needed.
- In-process DBMS’s, like SQLite or duckdb, run entirely on your
  computer. They’re great for working with large datasets where you’re
  the primary user.

## Play the dbduck database

### Create a database

``` r
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = file.path(workDir, "duckdb"))
```

### Load some data

We will use the `DBI::dbWriteTable()` function to write some ggplot
datasets into tables:

``` r
dbWriteTable(con, "mpg", ggplot2::mpg)
dbWriteTable(con, "diamonds", ggplot2::diamonds)
```

In practice, one can use `duckdb_read_csv()` and
`duckdb_register_arrow()` to quickly load data into duckdb database
without reading them into R.

### List tables in the database

``` r
dbListTables(con)
```

    ## [1] "diamonds" "mpg"

### Use dbplyr to interact with the database

To use dbplyr, we first need to use the `tbl()` function from dbplyr to
reference a table in the database:

``` r
mpg_db <- tbl(con, "mpg")
head(mpg_db)
```

    ## # Source:   SQL [?? x 11]
    ## # Database: DuckDB 1.4.3 [zhenguo@Linux 6.8.0-90-generic:R 4.5.2//tmp/RtmpNsXrPt/duckdb]
    ##   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class 
    ##   <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> 
    ## 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…
    ## 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…
    ## 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…
    ## 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…
    ## 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…
    ## 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…

First, many corporate databases are very large so you need some
hierarchy to keep all the tables organized. In that case you might need
to supply a schema, or a catalog and a schema, in order to pick the
table you’re interested in:

``` r
mpg_db <- tbl(con, in_schema("schema_name", "mpg"))
mpg_db <- tbl(con, in_catalog("catalog_name", "schema_name", "mpg"))
```

One can shw the SQL query generated by a dbplyr pipeline using the
`show_query()` function:

``` r
mpg_db %>%
  filter(cty > 20) %>%
  show_query()
```

    ## <SQL>
    ## SELECT mpg.*
    ## FROM mpg
    ## WHERE (cty > 20.0)

The dbplyr syntax is lazy, meaning it will not execute unless it is
necessary. To actually execute the query and return the results as a
data frame, one can use the `collect()` function:

``` r
mpg_db %>%
  filter(cty > 20) %>%
  collect()
```

    ## # A tibble: 45 × 11
    ##    manufacturer model  displ  year   cyl trans     drv     cty   hwy fl    class
    ##    <chr>        <chr>  <dbl> <int> <int> <chr>     <chr> <int> <int> <chr> <chr>
    ##  1 audi         a4       1.8  1999     4 manual(m… f        21    29 p     comp…
    ##  2 audi         a4       2    2008     4 auto(av)  f        21    30 p     comp…
    ##  3 chevrolet    malibu   2.4  2008     4 auto(l4)  f        22    30 r     mids…
    ##  4 honda        civic    1.6  1999     4 manual(m… f        28    33 r     subc…
    ##  5 honda        civic    1.6  1999     4 auto(l4)  f        24    32 r     subc…
    ##  6 honda        civic    1.6  1999     4 manual(m… f        25    32 r     subc…
    ##  7 honda        civic    1.6  1999     4 manual(m… f        23    29 p     subc…
    ##  8 honda        civic    1.6  1999     4 auto(l4)  f        24    32 r     subc…
    ##  9 honda        civic    1.8  2008     4 manual(m… f        26    34 r     subc…
    ## 10 honda        civic    1.8  2008     4 auto(l5)  f        25    36 r     subc…
    ## # ℹ 35 more rows

Typically, you’ll use dbplyr to select the data you want from the
database, performing basic filtering and aggregation using the
translations described below. Then, once you’re ready to analyse the
data with functions that are unique to R, you’ll collect() the data to
get an in-memory tibble, and continue your work with pure R code.

### Exectute sql queries

``` r
sql <- "
  SELECT carat, cut, clarity, color, price 
  FROM diamonds 
  WHERE price > 15000
"
dbGetQuery(con, sql) |>
  as_tibble()
```

    ## # A tibble: 1,655 × 5
    ##    carat cut       clarity color price
    ##    <dbl> <fct>     <fct>   <fct> <int>
    ##  1  1.54 Premium   VS2     E     15002
    ##  2  1.19 Ideal     VVS1    F     15005
    ##  3  2.1  Premium   SI1     I     15007
    ##  4  1.69 Ideal     SI1     D     15011
    ##  5  1.5  Very Good VVS2    G     15013
    ##  6  1.73 Very Good VS1     G     15014
    ##  7  2.02 Premium   SI2     G     15014
    ##  8  2.05 Very Good SI2     F     15017
    ##  9  1.5  Very Good VS1     F     15022
    ## 10  1.82 Very Good SI1     G     15025
    ## # ℹ 1,645 more rows

## Access Arrow object

You can also use dbplyr to access Arrow objects via duckdb. First, let’s
create an Arrow dataset:

``` r
library(arrow)
# create a sample arrow dataset
arrow_data_dir <- file.path(workDir, "arrow_data")
dir.create(arrow_data_dir, recursive = TRUE, showWarnings = FALSE)
write_dataset(
  iris,
  path = arrow_data_dir,
  format = "parquet"
)

# read the dataset back into an arrow object and then use to_duckdb() to convert to a duckdb table
iris_arrow <- open_dataset(arrow_data_dir)

iris_arrow |>
  to_duckdb() |>
  group_by(Species) |>
  summarise(Avg_Sepal_Length = mean(Sepal.Length)) |>
  head()
```

    ## Warning: Missing values are always removed in SQL aggregation functions.
    ## Use `na.rm = TRUE` to silence this warning
    ## This warning is displayed once every 8 hours.

    ## # Source:   SQL [?? x 2]
    ## # Database: DuckDB 1.4.3 [zhenguo@Linux 6.8.0-90-generic:R 4.5.2/:memory:]
    ##   Species    Avg_Sepal_Length
    ##   <chr>                 <dbl>
    ## 1 versicolor             5.94
    ## 2 virginica              6.59
    ## 3 setosa                 5.01

To convert a duckdb object into arrow object, one can use the
`to_arrow()` function. The conversion provides benefits that when some
features are not supported in either duckdb or arrow, one can switch to
the other backend to perform the operation. For example, DuckDB cannot
yet open multi-level files created by partitioning. To do this, you must
therefore go through Arrow and open the dataset, and then use
`to_duckdb` for following operations.

Let’s finally disconnet from the database:

``` r
dbDisconnect(con, shutdown = TRUE)
```

### Indexes or Keys

First of all, creating indexes or keys in database is compuationally
expensive, especially for large datasets.

The operations such as `rows_insert()`, `rows_update()`, and
`rows_upsert()` using the database backends (e.g. duckdb) require
primary/unique keys to be defined, otherwise they will not check for
duplicates, even though the argument `by` is supplied. If the Keys
exists, then the argument `by` is still needed, which should be the same
as or a subset of the Keys.

One can’t use `ALTER TABLE` statement to add primary/unique keys after
the table is created in duckdb. The keys must be defined when the table
is created.

## List of useful functions from dbplyr

Actually one can use almost all the dplyr functions as long as they are
implemented by the used database backends, such as duckdb.

| Function | Example | Description |
|----|----|----|
| tbl() | `tbl(con, "mpg")` | Reference a table in the database |
| pull() | tbl1 | \> pull(column_name) |
| in_schema() | `tbl(con, in_schema("schema_name", "mpg"))` | Reference a table in a specific schema |
| in_catalog() | `tbl(con, in_catalog("catalog_name", "schema_name", "mpg"))` | Reference a table in a specific catalog and schema |
| show_query() | `mpg_db %>% show_query()` | Show the SQL query generated by a dbplyr pipeline |
| collect() | `mpg_db %>% filter(cty > 20) %>% collect()` | Execute the query and return the results as a data frame |
| compute() | `mpg_db %>% filter(cty > 20) %>% compute(name="tbl_name", temporary = FALSE)` | Execute the query and store results in a remote table |
| copy_to() | `copy_to(con, tbl_query, "iris_table", overwrite = TRUE)` | Copy a query result to a remote table |
| left_join() | `tbl1 %>% left_join(tbl2, by = "id")` | Perform a left join between two tables |
| inner_join() | `tbl1 %>% inner_join(tbl2, by = "id")` | Perform an inner join between two tables |
| right_join() | `tbl1 %>% right_join(tbl2, by = "id")` | Perform a right join between two tables |
| full_join() | `tbl1 %>% full_join(tbl2, by = "id")` | Perform a full join between two tables |
| anti_join() | `tbl1 %>% anti_join(tbl2, by = "id")` | Return all rows in tb1 that don’t match in tb2 |
| semi_join() | `tbl1 %>% semi_join(tbl2, by = "id")` | Return all rows in tb1 that have a match in tb2 |
| pivot_wider() | `tbl %>% pivot_wider(names_from = "key", values_from = "value")` | Pivot a table from long to wide format |
| pivot_longer() | `tbl %>% pivot_longer(cols = c("col1", "col2"), names_to = "key", values_to = "value")` | Pivot a table from wide to long format |
| case_when() | `tbl %>% mutate(new_col = case_when(condition1 ~ value1, condition2 ~ value2))` | Create a new column based on multiple conditions |
| if_else() | `tbl %>% mutate(new_col = if_else(condition, true_value, false_value))` | Create a new column based on a single condition |
| union() | tb1 | \> union(tb2, copy=T) |
| union_all() | tb1 | \> union_all(tb2, copy=T) |
| intersect() | tb1 | \> intersect(tb2) |
| setdiff() | tb1 | \> setdiff(tb2) |

Functions affect rows:

| Function                   | Example | Description                          |
|----------------------------|---------|--------------------------------------|
| arrange()                  | tbl     | \> arrange(col1, desc(col2)          |
| distinct()                 | tbl     | \> distinct(col1, col2, .keep_all=T) |
| filter()                   | tbl     | \> filter(col1 \> 5, col2 == TRUE)   |
| slice_min()                | tbl     | \> slice_min(col1,n=2)               |
| slice_max()                | tbl     | \> slice_max(col1,n=2)               |
| slice_sample()             | tbl     | \> slice_sample(n=3)                 |
| slice_head(), slice_tail() | tbl     | \> slice_head()                      |

Functions affect columns:

| Function    | Example | Description                 |
|-------------|---------|-----------------------------|
| count()     | tbl     | \> count()                  |
| add_count() | tbl     | \> add_count()              |
| group_by()  | tbl     | \> group_by(col1, col2)     |
| summarize() | tbl     | \> summarize(n(),mean(col1) |
| do()        | tbl     | \> do()                     |

Functions to modify tables (can be in place)

| Function      | Example | Description |
|---------------|---------|-------------|
| rows_insert() |         |             |
| rows_update() |         |             |
| rows_upsert() |         |             |
| rows_delete() |         |             |
| rows_append() |         |             |
| rows_patch()  |         |             |

## List of useful functions from DBI

| Function | Description |
|----|----|
| dbConnect() | Connect to a database |
| dbDisconnect() | Disconnect from a database |
| dbListTables() | List all tables in the database |
| dbListFields() | List all fields (columns) in a table |
| dbReadTable() | Read an entire table into R as a data.frame |
| dbWriteTable() | Write an R data frame to a table in the database |
| dbAppendTable() | Append data to an existing table in the database |
| dbGetQuery() | Execute a SQL query and return the result as a data frame |
| dbExecute() | Execute a SQL statement without returning a result |
| dbRemoveTable() | Remove a table from the database |
| dbExistsTable() | Check if a table exists in the database |
| dbGetInfo() | Get information about the database connection |
| dbBegin(), dbCommit(), dbRollback() | Manage transactions (begin, commit, rollback) |
| dbSendQuery(), dbFetch(), dbClearResult() | Send a query, fetch results, and clear the result set |

## List of useful functions from duckdb

duckdb_read_csv() : read csv files directly into a duckdb database

## References

- dm: an R package to bridge the gap in the data pipeline between
  individual data frames and relational databases
  <https://dm.cynkra.com/>
- Manipulate big data with Arrow and Duckdb:
  <https://www.christophenicault.com/post/large_dataframe_arrow_duckdb/>

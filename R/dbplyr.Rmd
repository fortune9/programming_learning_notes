---
title: "Example code for using dbplyr arrow package to handle big dataset"
author: "Zhenguo Zhang"
date: "`r Sys.Date()`"
# use github format
output:
  github_document:
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message=FALSE)
workDir<-tempdir()
```

```{r}
library(DBI)
library(dbplyr)
library(tidyverse)
```


This document is based on the page https://r4ds.hadley.nz/databases.html

In R, there are two important packages to handle databases:

- DBI: a common low-level interface between R and database management systems
- dbplyr: a dplyr backend that translates your dplyr code to SQL and then executes
  them with DBI.
  
Databases are run by database management systems (DBMS’s for short), which come in three basic forms:

- Client-server DBMS’s run on a powerful central server, which you connect to from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.
- Cloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.
- In-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user.

## Play the dbduck database

### Create a database

```{r}
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = file.path(workDir, "duckdb"))
```

### Load some data

We will use the `DBI::dbWriteTable()` function to write some ggplot datasets into tables:

```{r}
dbWriteTable(con, "mpg", ggplot2::mpg)
dbWriteTable(con, "diamonds", ggplot2::diamonds)
```

In practice, one can use `duckdb_read_csv()` and `duckdb_register_arrow()` to quickly load data
into duckdb database without reading them into R. 


### List tables in the database

```{r}
dbListTables(con)
```

### Use dbplyr to interact with the database

To use dbplyr, we first need to use the `tbl()` function from dbplyr to reference a table in the database:

```{r}
mpg_db <- tbl(con, "mpg")
head(mpg_db)
```
 First, many corporate databases are very large so you need some hierarchy to keep all the tables organized. In that case you might need to supply a schema, or a catalog and a schema, in order to pick the table you’re interested in:

```{r, eval=FALSE}
mpg_db <- tbl(con, in_schema("schema_name", "mpg"))
mpg_db <- tbl(con, in_catalog("catalog_name", "schema_name", "mpg"))
```


One can shw the SQL query generated by a dbplyr pipeline using the `show_query()` function:

```{r}
mpg_db %>%
  filter(cty > 20) %>%
  show_query()
```

The dbplyr syntax is lazy, meaning it will not execute unless it is necessary.
To actually execute the query and return the results as a data frame, one can use the `collect()` function:

```{r}
mpg_db %>%
  filter(cty > 20) %>%
  collect()
```

Typically, you’ll use dbplyr to select the data you want from the database, performing basic filtering and aggregation using the translations described below. Then, once you’re ready to analyse the data with functions that are unique to R, you’ll collect() the data to get an in-memory tibble, and continue your work with pure R code.


### Exectute sql queries

```{r}
sql <- "
  SELECT carat, cut, clarity, color, price 
  FROM diamonds 
  WHERE price > 15000
"
dbGetQuery(con, sql) |>
  as_tibble()
```


## Access Arrow object

You can also use dbplyr to access Arrow objects via duckdb. First, let's create an Arrow dataset:

```{r}
library(arrow)
# create a sample arrow dataset
arrow_data_dir <- file.path(workDir, "arrow_data")
dir.create(arrow_data_dir, recursive = TRUE, showWarnings = FALSE)
write_dataset(
  iris,
  path = arrow_data_dir,
  format = "parquet"
)

# read the dataset back into an arrow object and then use to_duckdb() to convert to a duckdb table
iris_arrow <- open_dataset(arrow_data_dir)

iris_arrow |>
  to_duckdb() |>
  group_by(Species) |>
  summarise(Avg_Sepal_Length = mean(Sepal.Length)) |>
  head()
```

To convert a duckdb object into arrow object, one can use the `to_arrow()` function.
The conversion provides benefits that when some features are not supported in either duckdb or arrow, one can switch to the other backend to perform the operation.
For example,  DuckDB cannot yet open multi-level files created by partitioning. 
To do this, you must therefore go through Arrow and open the dataset, and then
use `to_duckdb` for following operations.


Let's finally disconnet from the database:

```{r}
dbDisconnect(con, shutdown = TRUE)
```

### Indexes or Keys

First of all, creating indexes or keys in database is compuationally
expensive, especially for large datasets.

The operations such as `rows_insert()`, `rows_update()`, and
`rows_upsert()` using the database backends (e.g. duckdb) require
primary/unique keys to be defined, otherwise they will not check for
duplicates, even though the argument `by` is supplied. If the Keys
exists, then the argument `by` is still needed, which should be the
same as or a subset of the Keys.

One can't use `ALTER TABLE` statement to add primary/unique keys after the
table is created in duckdb. The keys must be defined when the table is created.

## List of useful functions from dbplyr

Actually one can use almost all the dplyr functions as long as they
are implemented by the used database backends, such as duckdb.

| Function | Example | Description |
|----------|---------|-------------|
| tbl() | `tbl(con, "mpg")` | Reference a table in the database |
| pull() | tbl1 |> pull(column_name) | get a column into an R variable|
| in_schema() | `tbl(con, in_schema("schema_name", "mpg"))` | Reference a table in a specific schema |
| in_catalog() | `tbl(con, in_catalog("catalog_name", "schema_name", "mpg"))` | Reference a table in a specific catalog and schema |
| show_query() | `mpg_db %>% show_query()` | Show the SQL query generated by a dbplyr pipeline |
| collect() | `mpg_db %>% filter(cty > 20) %>% collect()` | Execute the query and return the results as a data frame |
| compute() | `mpg_db %>% filter(cty > 20) %>% compute(name="tbl_name", temporary = FALSE)` | Execute the query and store results in a remote table |
| copy_to() | `copy_to(con, tbl_query, "iris_table", overwrite = TRUE)` | Copy a query result to a remote table |
| left_join() | `tbl1 %>% left_join(tbl2, by = "id")` | Perform a left join between two tables |
| inner_join() | `tbl1 %>% inner_join(tbl2, by = "id")` | Perform an inner join between two tables |
| right_join() | `tbl1 %>% right_join(tbl2, by = "id")` | Perform a right join between two tables |
| full_join() | `tbl1 %>% full_join(tbl2, by = "id")` | Perform a full join between two tables |
| anti_join() | `tbl1 %>% anti_join(tbl2, by = "id")` | Return all rows in tb1 that don't match in tb2 |
| semi_join() | `tbl1 %>% semi_join(tbl2, by = "id")` | Return all rows in tb1 that have a match in tb2 |
| pivot_wider() | `tbl %>% pivot_wider(names_from = "key", values_from = "value")` | Pivot a table from long to wide format |
| pivot_longer() | `tbl %>% pivot_longer(cols = c("col1", "col2"), names_to = "key", values_to = "value")` | Pivot a table from wide to long format |
| case_when() | `tbl %>% mutate(new_col = case_when(condition1 ~ value1, condition2 ~ value2))` | Create a new column based on multiple conditions |
| if_else() | `tbl %>% mutate(new_col = if_else(condition, true_value, false_value))` | Create a new column based on a single condition |
| union() | tb1 |> union(tb2, copy=T) | get the unique set of two tables |
| union_all() | tb1 |> union_all(tb2, copy=T) | get all the rows of two tables, including duplicates |
| intersect() | tb1 |> intersect(tb2) | return the common rows of the two tables |
| setdiff() | tb1 |> setdiff(tb2) | return the rows specific to tb1 |


Functions affect rows:

| Function | Example | Description |
|----------|---------|-------------|
| arrange() | tbl |> arrange(col1, desc(col2) | sort rows based on specified columns |
| distinct() | tbl |> distinct(col1, col2, .keep_all=T) | get first row for each combination of the specified columns | 
| filter() | tbl |> filter(col1 > 5, col2 == TRUE) | filter rows based on conditions |
| slice_min() | tbl |> slice_min(col1,n=2) | select `n` rows which has minimal value on the specified columns. For grouped data, this is done for each group |
| slice_max() | tbl |> slice_max(col1,n=2) | select `n` rows which has maximal value on the specified columns |
| slice_sample() | tbl |> slice_sample(n=3) | randomly sample `n` rows or as a proportion of it. |
| slice_head(), slice_tail() | tbl |> slice_head() | get first or last rows (of each group if grouped data); may not implemented |

Functions affect columns:

| Function | Example | Description |
|----------|---------|-------------|
| count() | tbl |> count() | count all rows or a variable if specified |
| add_count() | tbl |> add_count() | add count in the output |
| group_by() | tbl |> group_by(col1, col2) | group data based on variables |
| summarize() | tbl |> summarize(n(),mean(col1) | summarize data for each group | 
| do() | tbl |> do() | perform any computation on remote backend |


Functions to modify tables (can be in place)

| Function | Example | Description |
|----------|---------|-------------|
| rows_insert() | |
| rows_update() | |
| rows_upsert() | |
| rows_delete() | |
| rows_append() | |
| rows_patch() | |


## List of useful functions from DBI

| Function                  | Description                                      |
|---------------------------|--------------------------------------------------|
| dbConnect()               | Connect to a database                            |
| dbDisconnect()            | Disconnect from a database                       |
| dbListTables()           | List all tables in the database                  |
| dbListFields()           | List all fields (columns) in a table             |
| dbReadTable()            | Read an entire table into R as a data.frame      |
| dbWriteTable()           | Write an R data frame to a table in the database |
| dbAppendTable()         | Append data to an existing table in the database  |
| dbGetQuery()             | Execute a SQL query and return the result as a data frame |
| dbExecute()              | Execute a SQL statement without returning a result |
| dbRemoveTable()          | Remove a table from the database                 |
| dbExistsTable()          | Check if a table exists in the database          |
| dbGetInfo()              | Get information about the database connection    |
| dbBegin(), dbCommit(), dbRollback() | Manage transactions (begin, commit, rollback) |
| dbSendQuery(), dbFetch(), dbClearResult() | Send a query, fetch results, and clear the result set |


## List of useful functions from duckdb

duckdb_read_csv() : read csv files directly into a duckdb database


## References

- dm: an R package to bridge the gap in the data pipeline between individual data frames and relational databases https://dm.cynkra.com/
- Manipulate big data with Arrow and Duckdb: https://www.christophenicault.com/post/large_dataframe_arrow_duckdb/


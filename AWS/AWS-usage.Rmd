---
title: "AWS usage"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
    toc: true
    toc_depth: 3
    fig_width: 7
    fig_height: 5
    pandoc_args: --webtex=http://chart.apis.google.com/chart?cht=tx&chl=
always_allow_html: true
---



## EBS volumes


### SSD volumes

Actually, reading and writing data from EBS depends on a number of factors.
One can find more info [here](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html).

The performance of gp2 volumes is tied to volume size, which determines the baseline performance level
of the volume and how quickly it accumulates I/O credits; larger volumes have higher baseline performance
 levels and accumulate I/O credits faster. I/O credits represent the available bandwidth that your gp2
volume can use to burst large amounts of I/O when more than the baseline performance is needed. 
The more credits your volume has for I/O, the more time it can burst beyond its baseline performance 
level and the better it performs when more performance is needed. 

Each volume receives an initial I/O credit balance of 5.4 million I/O credits, which is enough 
to sustain the maximum burst performance of 3,000 IOPS for at least 30 minutes. 
Volumes earn I/O credits at the baseline performance rate of 3 IOPS per GiB of volume size. 
For example, a 100 GiB gp2 volume has a baseline performance of 300 IOPS.

When your volume requires more than the baseline performance I/O level, it draws on I/O credits 
in the credit balance to burst to the required performance level, up to a maximum of 3,000 IOPS. 
 When your volume uses fewer I/O credits than it earns in a second, unused I/O credits are added
 to the I/O credit balance. The maximum I/O credit balance for a volume is equal to the initial
 credit balance (5.4 million I/O credits).

If your gp2 volume uses all of its I/O credit balance, the maximum IOPS performance of the volume
remains at the baseline IOPS performance level (the rate at which your volume earns credits). 
The volume's maximum throughput is reduced to the baseline IOPS multiplied by the maximum I/O size. 
Throughput can never exceed 250 MiB/s (for GP2 volumes).

When I/O demand drops below the baseline level and unused credits are added to the I/O credit balance,
the maximum IOPS performance of the volume again exceeds the baseline. For example, a 100 GiB gp2
volume with an empty credit balance has a baseline performance of 300 IOPS and a throughput limit
of 75 MiB/s (300 I/O operations per second * 256 KiB per I/O operation = 75 MiB/s). 


Provisioned IOPS SSD (io1 and io2) volumes are designed to meet the needs of I/O-intensive workloads,
particularly database workloads, that are sensitive to storage performance and consistency. 
Provisioned IOPS SSD volumes use a consistent IOPS rate, which you specify when you create the volume,
and Amazon EBS delivers the provisioned performance 99.9 percent of the time.


### HDD volumes

Throughput Optimized HDD (st1) volumes provide low-cost magnetic storage that defines performance 
in terms of throughput rather than IOPS. This volume type is a good fit for large, sequential
workloads such as Amazon EMR, ETL, data warehouses, and log processing.
This volume type is optimized for workloads involving large, sequential I/O, and we recommend
that customers with workloads performing small, random I/O use gp2.

For a 1-TiB st1 volume, burst throughput is limited to 250 MiB/s, the bucket fills with credits
at 40 MiB/s, and it can hold up to 1 TiB-worth of credits. Larger volumes scale these limits linearly,
with throughput capped at a maximum of 500 MiB/s. After the bucket is depleted, throughput is
limited to the baseline rate of 40 MiB/s per TiB. 




The following diagram shows the burst-bucket behavior for gp2.

### EFS volume

EFS volume is more expensive than EBS volume, it has the following advantages
and disadvantages:

- Advantage:
    + resize automatically based on the data written on it

- Disadvantages:
    + failure when writing massive data in parallel, such as writing nextflow output.
    + slow transfer across different aws regions.


## AWS Batch

Dos and Don'ts: https://aws.amazon.com/blogs/hpc/aws-batch-best-practices/

1. 



How to create compute environment: https://docs.aws.amazon.com/batch/latest/userguide/create-compute-environment.html


1. Compute environment (CE) creation

	- the instance types should share the same architecture, such as no mix between x86 and ARM architectures.
	- to use GPUs, one need include instances from p2, p3, p4, g3, g3s, and g4 families.
	- Allocation strategy: BEST_FIT_PROGRESSIVE is better choice for on-demand EC2 instances, and SPOT_CAPACITY_OPTIMIZED
	  is better choice for EC2 spot instances.
	- one can add a launch template in CE, which can mount volumes, add EC2 keypair, AMIs, etc. See https://docs.aws.amazon.com/batch/latest/userguide/launch-templates.html
	- one can use user-specified AMI, and it must match the instance types in the CE, for example, A1 instances
	  should use AMIs supporting ARM instances.
    - 

2. One can use Launch template to override/add information in the AMI used for compute environment,
    such as userkeypair, mount volumes, etc. See [here](https://docs.aws.amazon.com/batch/latest/userguide/launch-templates.html) for more information.


3. Compute resource AMI: https://docs.aws.amazon.com/batch/latest/userguide/create-batch-ami.html, https://docs.aws.amazon.com/batch/latest/userguide/compute_resource_AMIs.html#batch-ami-spec


4. Allocation strategy: best fit to AWS Batch means the least number of instances capable of running the workload, at the lowest cost
    In general, we recommend the best fit strategy only when you want the lowest cost for your instance, and you’re willing to trade cost for throughput and availability.

    The best fit progressive strategy allows you to let AWS Batch choose the best fit instance for your workload 
    (based on your jobs’ vCPU and memory requirements). In this context, “best fit” means AWS Batch provisions the
    least number of instances capable of running your jobs at the lowest cost.
    
    See more [here](https://aws.amazon.com/blogs/compute/optimizing-for-cost-availability-and-throughput-by-selecting-your-aws-batch-allocation-strategy/)
    
5. AWS Batch best practices: https://aws.amazon.com/blogs/hpc/aws-batch-best-practices/
    
    - To shorten the startup time of a job, one should use smaller container with more layers, because when pulling
        the docker image, each layer is pulled independently in a thread in parallel. Small containers and multiple
        layers make image pulling much faster.
    - there are limits on CPUs, disk storage per service account.
    - Scenarios when AWS batch isn't a good choice: very-short period jobs due to overhead.
    - 
  



## S3

### Mount S3 bucket

To mount an S3 bucket to a local file system, one can use the tool
`goofys`, like

```bash
mkdir -p /mnt/s3/my_bucket
goofys <my_s3_bucket> /mnt/s3/my_bucket
```

Note that:

- This command can be run as both regular and root users, which determine
    the default permissions on the mounted folder. One may tune this by
    using the option --uid and --gid.

- To allow other users (not the one mounting the bucket) to access the folders,
    one need to use the option `-o allow_other`. However, this option is only allowed
    for root user; to let other users such as `ubuntu` to use this option, one
    need to enable/uncomment the option */etc/fuse.conf*. The advantage of the latter
    is that the mounting user can access write/read the mounted bucket, and the
    other regular users can only read the bucket.
    


### AWS CLI S3 Configuration (AWS CLI version 1)

These configurations affect all `aws s3` commands, including `cp`, `sync`, `mv`, and `rm`.
Normally, one need not set these, but for performance or adapting to a specific environment,
they can be modified.

These values need be set at the top level of the s3 key in the AWS config file (default is `~/.aws/config`).
Below is an example:

```
[profile development]
aws_access_key_id=foo
aws_secret_access_key=bar
s3 =
  max_concurrent_requests = 20
  max_queue_size = 10000
  multipart_threshold = 64MB
  multipart_chunksize = 16MB
  max_bandwidth = 50MB/s
  use_accelerate_endpoint = true
  addressing_style = path
```

One can also set these values in command line, such as:

```
## set in default profile
aws configure set default.s3.max_concurrent_requests 20
aws configure set default.s3.max_queue_size 10000
aws configure set default.s3.multipart_threshold 64MB
## or in a different profile
aws configure set s3.max_concurrent_requests 20 --profile "test-profile"
aws configure set s3.max_queue_size 10000 --profile "test-profile"
aws configure set s3.multipart_threshold 64MB --profile "test-profile"
```


### Settings for `aws s3` commands

- max_concurrent_requests: The maximum number of parallel transfers at a certain time. 
    One can lower this number if the system (I/O, memory, bandwidth, etc) is overwhelmed by the transfer.
    One can increase this number if he wants to use more bandwidth with faster transfer.
    Default 10.
    
- max_queue_size: The maximum number of tasks in the task queue maintained by aws cli. aws cli uses a
    producer-consumer model to schedule tasks (each task can be a `PutObjectTask`, `GetObjectTask`, or `UploadPartTask`)
    entered in a queue. The queue increases when new tasks are created, and this number puts a limit on
    the total number of tasks in a queue at a time to avoid unbounded growth, given that task consuming
    is slower than task creation. Increase this number will have more tasks in the queue and also require
    more memory. Default 1000.
    
- multipart_threshold: The size threshold the CLI uses for multipart transfers of individual files, 
    If a file has a size smaller than this value, multipart transfer will not be triggered. The provided
    value can be in bytes (e.g., 1024000) or with size suffix (e.g., 100KB, 10MB, 2GB, 1TB).
    
- multipart_chunksize: When using multipart transfers, this is the chunk size that the CLI uses 
    for multipart transfers of individual files. Default 8MB, Minimum 5MB.
    
- max_bandwidth: The maximum bandwidth that will be consumed for uploading and downloading data to and from Amazon S3.
    This parameter thus only affect upload and download tasks, and has no effect copy or delete
    operations as they happen on the server side only.
    
  

### Settings for `aws s3` and `aws s3api` commands

- use_accelerate_endpoint: Use the Amazon S3 Accelerate endpoint for all s3 and s3api commands. 
    You must first enable S3 Accelerate on your bucket before attempting to use the endpoint. 
    This is mutually exclusive with the use_dualstack_endpoint option. When this is true, 
    All request will be sent using the virtual style of bucket addressing: my-bucket.s3-accelerate.amazonaws.com.
    Default false.
    
- use_dualstack_endpoint: Use the Amazon S3 dual IPv4 / IPv6 endpoint for all `s3` and `s3api` commands.
    This is mutually exclusive with the use_accelerate_endpoint option. Default false.
    
- addressing_style: Specifies which addressing style to use. This controls if the bucket name is in the 
    hostname or part of the URL. Value values are: path, virtual, and auto. The default value is auto.
    
- payload_signing_enabled: Refers to whether or not to SHA256 sign sigv4 payloads. By default, this is
    disabled for streaming uploads (UploadPart and PutObject) when using https.


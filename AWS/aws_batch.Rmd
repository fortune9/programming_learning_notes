

Batch is organised in *Compute Environments*, *Job queues*, *Job definitions* and *Jobs*.

* Compute Environment:  allows you to define the computing resources required for a 
	specific workload (type). You can specify the minimum and maximum number of CPUs
	that can be allocated, the EC2 provisioning model (On-demand or Spot), the AMI
	to be used and the allowed instance types. This is the key how the computing
	resources are assigned to a job queue.

* Job queue: allows you to bind a specific task to one or more Compute Environments.

* Job definition: a template for one or more jobs in your workload. This is required 
	to specify the Docker image to be used in running a particular task along with 
	other requirements such as the container mount points, the number of CPUs, the
	amount of memory and the number of retries in case of job failure.

* Job: binds a Job definition to a specific Job queue and allows you to specify the
	actual task command to be executed in the container.

The job input and output data management is delegated to the user. This means that
if you only use Batch API/tools you will need to take care to stage the input data
from a S3 bucket (or a different source) and upload the results to a persistent
storage location.

This could turn out to be cumbersome in complex workflows with a large number of tasks
and above all it makes it difficult to deploy the same applications across different
infrastructure. 

Nextflow streamlines the use of AWS Batch by smoothly integrating it in its workflow
processing model and enabling transparent interoperability with other systems. After
setup computing environment and job queues in an AWS account, Nextflow takes care to
create the required Job Definitions and Job requests as needed. 
